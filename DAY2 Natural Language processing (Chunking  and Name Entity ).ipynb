{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43cdd4d7",
   "metadata": {},
   "source": [
    " ## Chunking is a process of extracting phrases (chunks) from unstructured text based on certain patterns or rules.\n",
    " - A chunk grammar is a combination of rules on how sentences should be chunked. It often uses regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cde4aff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, RegexpParser\n",
    "\n",
    "# Sample text\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Perform part-of-speech tagging\n",
    "tagged_tokens = pos_tag(tokens)\n",
    "print(tagged_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89c6ca5",
   "metadata": {},
   "source": [
    "### Steps for Chunking\n",
    "\n",
    "- We define the chunk grammar using regular expressions. The grammar specifies patterns that indicate how chunks should be formed. In this example, we define a simple grammar to chunk noun phrases (NP) consisting of optional determiners (DT), adjectives (JJ), and nouns (NN).\n",
    "\n",
    "- Chunk Parser: We create a chunk parser using the defined gramma\n",
    "\n",
    "-  Apply the chunk parser to the tagged tokens, which identifies and groups tokens according to the patterns specified in the grammar.\n",
    "\n",
    "- Print the chunked tokens, which represent the identified phrases based on the chunking rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8fab0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP The/DT quick/JJ brown/NN)\n",
      "  (NP fox/NN)\n",
      "  jumps/VBZ\n",
      "  over/IN\n",
      "  (NP the/DT lazy/JJ dog/NN))\n"
     ]
    }
   ],
   "source": [
    "# Define chunk grammar using regular expressions\n",
    "chunk_grammar = r\"\"\"\n",
    "    NP: {<DT>?<JJ>*<NN>}   # Chunk sequences of DT, JJ, and NN\n",
    "    \"\"\"\n",
    "\n",
    "# Create a chunk parser using the defined grammar\n",
    "chunk_parser = RegexpParser(chunk_grammar)\n",
    "\n",
    "# Apply chunking\n",
    "chunked_tokens = chunk_parser.parse(tagged_tokens)\n",
    "\n",
    "# Print the chunked tokens\n",
    "print(chunked_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b330643",
   "metadata": {},
   "source": [
    "## Rules\n",
    "- According to the rule you created, your chunks:\n",
    "- Start with an optional (?) determiner ('DT')\n",
    "- Can have any number (*) of adjectives (JJ)\n",
    "- End with a noun ()\n",
    "\n",
    "## Let's create another example for chunking, this time focusing on verb phrases (VP) in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b014f310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Suyashi/NNP\n",
      "  has/VBZ\n",
      "  a/DT\n",
      "  Rabbit/NNP\n",
      "  that/WDT\n",
      "  ran/VBD\n",
      "  from/IN\n",
      "  the/DT\n",
      "  Table/NN\n",
      "  ./.\n",
      "  She/PRP\n",
      "  bought/VBD\n",
      "  it/PRP\n",
      "  from/IN\n",
      "  Isha/NNP\n",
      "  ./.\n",
      "  The/DT\n",
      "  Jumping/NNP\n",
      "  is/VBZ\n",
      "  best/JJS\n",
      "  Habit/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  Rabbit/NNP)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, RegexpParser\n",
    "\n",
    "# Sample text\n",
    "text = \"Suyashi has a Rabbit that ran from the Table. She bought it from Isha . The Jumping is best Habit of the Rabbit\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Perform part-of-speech tagging\n",
    "word_tokens_text = pos_tag(tokens)\n",
    "\n",
    "# Define chunk grammar using regular expressions\n",
    "chunk_grammar = r\"\"\"\n",
    "    VP: {<VB.*><DT>?<JJ>*<NN>}   # Chunk sequences of verbs, determiners, adjectives, and nouns\n",
    "    \"\"\"\n",
    "\n",
    "# Create a chunk parser using the defined grammar\n",
    "chunk_parser = RegexpParser(chunk_grammar)\n",
    "\n",
    "# Apply chunking\n",
    "chunked_tokens = chunk_parser.parse(word_tokens_text)\n",
    "\n",
    "# Print the chunked tokens\n",
    "print(chunked_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae3b226",
   "metadata": {},
   "source": [
    "## Quick Practice\n",
    "Let's create another example for chunking, this time focusing on extracting noun phrases (NP) along with prepositional phrases (PP) from a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045f810c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, RegexpParser\n",
    "\n",
    "# Sample text\n",
    "text = \"Ram Loves his Life. He have a cat named RUMMY\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Perform part-of-speech tagging\n",
    "tagged_tokens = pos_tag(tokens)\n",
    "\n",
    "# Define chunk grammar using regular expressions\n",
    "chunk_grammar = r\"\"\"\n",
    "    NP: {<DT>?<JJ>*<NN>}    # Chunk noun phrases\n",
    "    PP: {<IN><NP>}           # Chunk prepositional phrases\n",
    "    \"\"\"\n",
    "\n",
    "# Create a chunk parser using the defined grammar\n",
    "chunk_parser = RegexpParser(chunk_grammar)\n",
    "\n",
    "# Apply chunking\n",
    "chunked_tokens = chunk_parser.parse(tagged_tokens)\n",
    "\n",
    "# Print the chunked tokens\n",
    "print(chunked_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0cff1f",
   "metadata": {},
   "source": [
    "## Using Named Entity Recognition (NER)\n",
    "- Named entities are noun phrases that refer to specific locations, people, organizations, and so on.\n",
    "- With named entity recognition, you can find the named entities in your texts and also determine what kind of named entity they are\n",
    "- you can use nltk.ne_chunk() to recognize named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be42b12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  You/PRP\n",
      "  know/VBP\n",
      "  (PERSON Ashi/NNP)\n",
      "  ,/,\n",
      "  she/PRP\n",
      "  works/VBZ\n",
      "  in/IN\n",
      "  (ORGANIZATION ABC/NNP)\n",
      "  pvt/NN\n",
      "  Lt./NNP\n",
      "  (GPE India/NNP)\n",
      "  ,/,\n",
      "  and/CC\n",
      "  its/PRP$\n",
      "  (ORGANIZATION CEO/NNP Rommy/NNP)\n",
      "  is/VBZ\n",
      "  from/IN\n",
      "  (GPE Australia/NNP)\n",
      "  ./.\n",
      "  (PERSON Rabbit/NNP)\n",
      "  plays/VBZ\n",
      "  with/IN\n",
      "  (ORGANIZATION Cat/NNP))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, ne_chunk\n",
    "\n",
    "# Sample text\n",
    "text = \"You know Ashi, she works  in ABC pvt Lt. India, and its CEO  Rommy  is from Australia. Rabbit plays with Cat\"\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Perform part-of-speech tagging\n",
    "tagged_tokens = pos_tag(tokens)\n",
    "\n",
    "# Perform named entity recognition\n",
    "named_entities = ne_chunk(tagged_tokens)  #This function identifies named entities in the text based on the part-of-speech tags.\n",
    "\n",
    "# Print the named entities\n",
    "print(named_entities) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24c94234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asha - PERSON\n",
      "Bali - GPE\n",
      "Kipy - PERSON\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 10.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag, ne_chunk\n",
    "\n",
    "# Sample text\n",
    "text = \"The teacher have a kid named Asha. They stay in Bali. They have a pet named Kipy.\"\n",
    "\n",
    "# Tokenize the text into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Initialize a list to store named entities\n",
    "named_entities = []\n",
    "\n",
    "# Iterate through each sentence\n",
    "for sentence in sentences:\n",
    "    # Tokenize the sentence into words\n",
    "    tokens = word_tokenize(sentence)\n",
    "    # Perform part-of-speech tagging\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    # Perform named entity recognition\n",
    "    named_entities.extend(ne_chunk(tagged_tokens))\n",
    "\n",
    "# Print the named entities\n",
    "for entity in named_entities:\n",
    "    if hasattr(entity, 'label'):\n",
    "        print(' '.join(c[0] for c in entity.leaves()), '-', entity.label())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bcfbd3",
   "metadata": {},
   "source": [
    "- Lambda functions are typically used for simple operations on single elements of a list, not for complex operations involving iteration, tokenization, tagging, and parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2ec64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag, ne_chunk\n",
    "\n",
    "# Sample text\n",
    "text = \"The teacher have a kid named Asha. They stay in Bali. They have a pet named Kipy.\"\n",
    "\n",
    "# Tokenize the text into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Perform named entity recognition for each sentence and flatten the result\n",
    "named_entities = [entity for sentence in sentences \n",
    "                        for entity in ne_chunk(pos_tag(word_tokenize(sentence))) \n",
    "                            if hasattr(entity, 'label')]\n",
    "\n",
    "# Print the named entities\n",
    "for entity in named_entities:\n",
    "    print(' '.join(c[0] for c in entity.leaves()), '-', entity.label())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fb5367",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER) with Custom Entities:\n",
    "- Extracting custom named entities from text using regular expressions and NLTK's NER capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa240079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, ne_chunk\n",
    "\n",
    "# Sample text\n",
    "text = \" Tina and Roohi are best Friends. They work in same company. They stay in America\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Perform part-of-speech tagging\n",
    "tagged_tokens = pos_tag(tokens)\n",
    "\n",
    "# Define a custom named entity chunker\n",
    "chunk_rule = r\"NE: {<NNP>+}\"\n",
    "custom_chunker = nltk.RegexpParser(chunk_rule)\n",
    "\n",
    "# Apply custom chunker\n",
    "custom_named_entities = custom_chunker.parse(tagged_tokens)\n",
    "\n",
    "# Print custom named entities\n",
    "print(custom_named_entities)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
